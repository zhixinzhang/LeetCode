背景是，client通过robinhood来place limited order，robinhood 通过call第三方market的API，
得到stock的具体信息，最后完成交易。面试官没直接给出具体的需求，我大概通过交流概括了一下：
1. support placing limited orders: during market hours, expire at each trading day
2. support accounting logic: to ensure the order is valid and consistent
3. ‍‌‌‌‌‍‌‌‍‍‌‌‍‌‌‍‌‍‌support multiple/concurrent orders


Hi Kamal, Before we start the design I would l like to go through the design process.
I will spend 5 to 10 minutes to talk about the design questions 
and both function and non-function requirements. Then maybe spend another 
10 minutes to talk about the system traffic estimation and DB selection and schema design.
 For high-level design maybe it will take 20 minutes, then hopefully 
we have 20 minutes left to deep dive into the important components. is that ok with you?

Can you tell me more about this system? Like who are our users and what features we need to build
I would like to clarify the requirements of the system and the functionality we need.

We need to design a stock exchange system like Robinhood. For this system user can buy and 
sell stock. User also can check order history and account info. 
If the order didn't finished by market time, then we should drop the order. 
We need to make sure the order is valid

1. How many different stocks we have in system,  1000 stocks? 
2. do we need to consider margin call account?  
3, do we support partial order? 

That's the function requirements for the system.
Let us talk about non-function requirements. 
*Non-function Req : 
keep the system highly available,
saclable, 
reliability
strong consistency
fault tolerance, 
fairness trading

I think That's the function and non-function requirements for our system,
do you have any questions or concerns here? 

Great, I would like to talk about throughput and how many 
orders we're going to store in DB.
* Traffic/DB size Estimation:

Can I assume that we have 6M DAU and each user will 
send 10 requests to our system every day 
the requests can like buy or sell or check order

Traffic Estimation and DB size
*6M  DAU -> 10 orders / per day 
*60 M orders   60M / 30K -> 2M / 1K -> 2K QPS for (buy and sell) 
*QPS approximately equal to 2K QPS from user sides 
*one server can host 100 qps -> 20 server
*1 Order =1 KB -> 60M KB = 60000 MB -> 60GB / every day

And If our Database starts to grow too large, we might consider only storing a 
limited time period of data in the database, 
while storing the rest in a data warehouse such as 
*Redshift/S3 

do you agree with?

We can use cloud computing service like
AWS/GCP/Azure to help us build up system. We don't want to buy hardware and maintain it
And Currently, aws is the most popular cloud service so 
if you agree I will chose aws as our Cloud service.
So far, is that ok with you?

Greate, before we jump to high level design, we can talk about what kind
of database we choose for our system and maybe we can design the job schema here. 
Based on requirements, that the backend database shoule be horizontally scalable 
In our user case, complex relational queries are not used and all data is structured. 
Most data access can be 
described as primary-key queries (e.g. given a job ID, get job status). 
Also, the issues of strong consistency and transactions are importance.
So I'm going to use Mysql database to store Order data.  Both SQL and NoSQL 
databases can used to store User Data.
*MySQL in here. 
For DB design, we should keep in mind that we maybe need to shard table 
to handle high-frequency read and write operations

Ok, here We need a table that keeps track of order metadata such as user_id, 
stock name 
* Order table {
    order_id: 
    user_id:
    stock_name:
    price:
    initial_shares: 10
    finished_shares: 6
    left_shares: 4
    status: new, executing, finished, expired
    create_time: 
    retry_count:
    shard_key: stock name
}
*user table {
}

I think that's the main tables and attributes we should have in our Order DB. 
we can add other attributes after high level design, if those attributes not enough 
for our system.
Any questions here? 

Ok, can we start high level design. 
In genreal, We have at least 3 big services in our stock exchange system. 
* Order Capture, * order executer Service *retrieve Service: 
we have Web Service, it's  to receive requests from user and store the order to DB. 
we have order executer Service to match buying and selling orders. 
And we have a retrieve service. it's will be very high frequency to retrieve
order from a third-party 
 
* Order Capture, * order executer Service *retrieve Service: 


* User 
1. first of all, we have a user and this user will send requests to our backend
service. because we have a high qps so we can use a * Load balancer to split 
the requests to our service. 

* Order Capture service (1k qps in here)
2. this service main function is to receive new orders and store the order to DB

*Order DB
3. we have a Order DB here, based on our discussion we may need to shard 
and replcicate the DB for system highly available and saclable. 
we can left it after we finished high level design.

* Order executer Services (20K orders)
4, we have a Order executer Service. This service is to 
match different stock order and send match order to third party like them
if the order executed successful then we need to update order data and update w
user account.

* SNS SQS
5. 
we want a buffer that holds 
all pending orders at peak hours. Secondly, we want to decouples the two services.
and After we receive a order, we need to make sure that the order data is
never get lost before we Execute the order. We can use Message queue to 
help us guarantees order delivery and  optimize data flow.

SQS is a Message queue, it's enable asynchronous communication. Producers can 
add requests to the queue without waiting for them to be processed. And SNS is a topic,
it's will send Stock to specific SQS


6. The data worflow in here is like order producer services send orders to SNS, 
and SNS will send order based on stock name, we can 1 SQS in here.
*SQS(Appl)  SQS(Meta)

OK 
*Order executor cluster
7. We have a order worker cluster, the order worker cluster is used to 
match orders , each order worker only handle one specific stock. So we have at lease 1000 
workers. For example, we have wokerA to match buy and sell Appl stock.
When we matched the order then we need to submit our new order to third party
if we got response then we will update our orders data.
Also, we need some backup workers in here incase some workers fail, and 
we better need to monitor the worker status. We can left this part after we finished high-level
design 

* retrieve Service
8. For retrieve Services, it will help us to high frequently get specific stock 
orders from a third broker.
We use similar data workflow liked we had for order capture. 
* SNS, SQS

when we retrieved a order, then we will send them to order worker cluster,


Other than that we should have one more notification services
* Order notification services and SQS
9.this service will receive order results from order worker cluster
and update order results to DB and update user account. Additionally this service will 
send a notification to user let them know, hey we finished your order 


This is the high-level design. I think it's can satisfy the Function requirements.
but we still have some bottlenecks on DB and order worker sides. 
For highly available and reliable we definitely need to replicate the DB 
and shard Order table to reduce index size 
and improve reading speed. Also, we have to make sure that every stock order 
only send to healthy order worker, otherwise, the order could be lost or 
pending for a long time
We are a finical and investment system, so we also need 
to make sure the strong transactions
between the user account and order. 
If you agree I can introduce important components one by one.

1. For example, we only have one DB, maybe we need to replicate DB in case the DB server down.
2. If we have 10B jobs how to improve the DB read speed? maybe we need sharding db and for 
every job producer service just search one specific shared partition.

3. if one job executer dead, then how can we keep make sure not lost job and if we have 
100M jobs, how to hanlde the spike

I can talk about the details of each important component?
1. if one job executer dead, then how can we keep make sure not lost job and if we have 
100M jobs, how to hanlde the spike

1. What if DB failed or how to improve DB reading performance 
Currently, we are using Mysql Databse to store Job data. we only have one single
 DB in here. to avoid DB failed and imprve db reading performance,
  we can replcicate Job Database.
*We use master-slave pattern
we have one master DB to handle write requests and we have duplicated 
DB to handle reades requests. If our master Job DB down then we can allow one slave DB to handle the write requests
until the master db recovery. We scrifiy the consistency if we use master-slave pattern
And as we mentioned, our data keep increasing so maybe we need to 
shard Job DB to improve our query and write performance. 
For example, we can use execution_time as shard key......



So for high avaiablility and fault-tolerance one service not 
enough to handle so many jobs
then we can have some *Job producer wokers 
to retrieve part of jobs from DB, 

2. what if job executor down?
Good question, if one of job executor down then we should ask other backup or available executors to pickup the jobs.  
To reduce cases where tasks run on dead servers
we need an executor health service, for each executor will send a heartbeat to executor health service every 5/10 seconds. 
And also we have a db redis/db to store 
executor data like {ip, last_heartbeat_time}, The executor health service will keep checking the executor data, 
if executor last_heartbeat_time expire a 10 seconds then 
executor health service will try to connect to executor 3 times, if it's failed then we mark it dead. we only send live executor to SNS. 
Use ElastiCache(Redis) to store job when the job send from SNS

3. what if Job Scheduling service down? 
To make sure a complete work assignment, we can borrow some ideas 
from MapReduce, where a master is used to assign and monitor workers. 
If a worker dies, the master will resend its work to some other nodes. 
An additional local database is used so that no job is scheduled twice. 
When a job is pushed to the queue, an entry is created in the local DB 
with 2 minutes of expiration time. 
If the original handler of the record dies and the shard is handed over 
to another worker, the new worker will skip tasks that exist in the 
database.

4. what Load balancer you want to choose?
Least Connection Method considers the current load on a server and aims to improve performance. it dose this by sending requests to the server 
with the least number of active connections.
Round robin is a simple load balancing algorithm that directs client requests to different servers based on a rotating list.


Data flow
With all the wrinkles in the high-level design address, we can finally come up with the data flow of the system:

Create/delete job/Retrieve history

The client sends out an RPC call to Web Service.
One of the RPC servers queries the database using the provided partition key and return the result
Schedule a job

Master

Every minute, the master node creates an 
authoritative UNIX timestamp and assigns a shard ID 
(see details for more) to each worker along with the timestamp
Check worker health regularly. If it dies, reassign its work to others
Worker

The worker queries the database with the timestamp and shard ID.
For each row, send it to the queue if it has not been scheduled (see details for more)
Execute a job

Orchestrator

A group of orchestrators consumes messages from the queue
Given a message, find one worker with the least workload. Assign the job to the worker
Commit the index, repeat steps 1 to 3
Worker

The worker regularly update the local database with its timestamp
Health Checker

Scans the local database ~ 10 seconds
If any row hasn’t been updated in ~30 seconds, retry it by pushing the job ID to the queue

We use Message Queue in here. 
Be able to scale the consumer and producer nodes independently (in Kafka we have Topics which can be 
horizontally partitioned and scale by that)
We decouple the consumer and producer from each other
Lower latency for the producer (doesn't have to wait for a response)
Durability and Reliability: When a Consumer Node crashes another Node can process the Message 
which otherwise would be lost (see Offset in Kafka). The Messages are persisted.

Amazon SNS with SQS is called fanout pattern. In this pattern, a message published to an SNS topic is 
distributed to multiple SQS queues in parallel and SQS queue assure persistence, because SQS has retention
 policy. It can persist message up to 14 days(defult 4 days).
 Amazon SQS with SNS can achieved highly throughput parallel streaming and can replace Apache Kafka.

The benefit for Message Queues
Message queues enable asynchronous communication. Producers can add requests to the queue without waiting for them to be processed. 
Consumers process messages only when they are available. No component in the system is ever stalled waiting for another, optimizing data flow.
Queues make your data persistent, and reduce the errors that happen when different parts of your system go offline
Message queues make it possible to scale precisely where you need to

DB

Reduced index size - Since the tables are divided and distributed into multiple servers, the total number of rows in each table in each database is reduced. This reduces index size, which generally improves search performance.


我用了MQ 有一个followup是如何来控制需要多少了consumers
问了如何来实现auto retry如果一个job failed了
问了如果某一个时间段 有很多jobs 需要怎么处理
问了如果在sch‍‌‌‌‌‍‌‌‍‍‌‌‍‌‌‍‌‍‌edule的时候需要保证high concurrency 怎么处理
比如说job dispatcher 挂了怎么办？
1. job dispatcher 有主从备份
2. job dispatcher 有sharding 如果挂了用类似consistent hashing的方式接管别的任务分发
问如果一个任务被执行多次怎么办？
1.要确保worker idempotent， 在任务前查库或者有fancy token的方式来确保不会重复提交


this is really low operation this is only going to run like once a minute or
55:08
something and it's just going to a single key range query so that's it's
55:13
not it's not even a full service it's just a Lambda function or a uh so you can you can set up ews Lambda
55:20
to um be triggered by a uh time just like a
55:25
Cron job and so that's what I envisioned for this thing um I think I already brought up that

DB (But yes, having a synchronous slave reduces the availability a bit. We shift from AP (CAP theorem) to CP a bit so it is a trade-off.)